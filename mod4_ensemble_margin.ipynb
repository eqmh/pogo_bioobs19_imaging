{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Margin and Ensemble Classifiers\n",
    "\n",
    "Before the popularization of deep learning, many applied automatic classification algorithms were variations on ensemble or margin classifiers. Both types of classifiers operate on pre-defined features. As we will see later, this is fundamentally different from neural networks which can learn directly from the images. For now, we will make use of the features pulled from SPC data in the last module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import skimage\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.join(os.getcwd(),'utilities'))\n",
    "from display_utils import make_confmat, tile_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important new toolkit we are importing here is *sklearn*, short for scikit-image. It contains most of the tools we will use to explore margin and ensemble classifiers.\n",
    "\n",
    "## Importing features, dividing into training and test sets\n",
    "\n",
    "For all the techinques discussed in the rest of this module, we will make use of the same features we computed before. We will also need to divide it into seperate sets for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data. first get all the file paths\n",
    "base_ptf = \"../Data/pogo_bioobs_2019/SPC_manual_labels_features/\"\n",
    "ptf = glob.glob(os.path.join(base_ptf, \"*.csv\"))\n",
    "\n",
    "# initalize a dictionary for the data. This will contain all the file paths and the associated features\n",
    "data = dict()\n",
    "\n",
    "# we will also create a flag and a listto give the labels a numeric value\n",
    "flag = 0\n",
    "cls_names = []\n",
    "\n",
    "for line in ptf:\n",
    "\n",
    "    # read in the data, but skip the image path\n",
    "    temp = np.genfromtxt(line, usecols=range(1,71),delimiter=\",\")\n",
    "    \n",
    "    # get the image path, making sure to specify that the data type is string\n",
    "    temp_path = np.genfromtxt(line, usecols= [0], delimiter=\",\", dtype=np.str)\n",
    "    \n",
    "    # for now, we will ignore any classes with fewer than 10 samples\n",
    "    if 10 < temp.shape[0]:\n",
    "        \n",
    "        # now we are creating a \"nested dictionary.\" Each element is referenced by the image id and contains the features\n",
    "        # and numeric class label\n",
    "        for img, feats in zip(temp_path, temp):\n",
    "            data[img] = {'features': feats, 'class': flag}\n",
    "            \n",
    "        # create a list of the names of the categories and the associated numbers\n",
    "        name = line.split('/')[-1].split('_')[0]\n",
    "        print(\"class\", str(flag), \":\", name, \n",
    "              \", num images:\", str(temp.shape[0]))\n",
    "        cls_names.append((flag, name))\n",
    "        \n",
    "        flag+=1\n",
    "\n",
    "print(\"Total class:\", str(flag), \", Total images:\", str(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 38 classes, comprising a total of 20678 samples. The nested dictionary is how we will interact with the data. Each data point is identified by its image ID that is saved as a dictionary key.\n",
    "\n",
    "Note that we are using python 3.6 which by default preserves the order of the dictionary. That is, the order of the key-value pairs will remain the same as how they were inserted into the dictionary no matter what. If for some reason you use an earlier version of python, be aware that the order may not be preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get a list of all the dictionary keys use Python's built in list command and the dictionary method keys()\n",
    "img_ids = list(data.keys())\n",
    "\n",
    "print(\"the first sample is:\", img_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this later to display images after they have been classifier. We can call up the information from that particular image by calling that key's associated values from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data related to a particular sample put the key in square brackets\n",
    "# remember, the data is store as a dictionary itself. We can also print those keys in the same way\n",
    "\n",
    "print(img_ids[0], \"has two keys that can be referenced:\", data[img_ids[0]].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the features and the class of that first sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the class\n",
    "print(\"the numeric class is:\", data[img_ids[0]]['class'])\n",
    "\n",
    "# and the features\n",
    "print(\"the features are:\",  data[img_ids[0]]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data in the workspace, we need to divide it up for training and testing. That is we need to seperate out a subset of the training data to use as an independent set to assess how well the classifier is doing. \n",
    "\n",
    "To do the data dictionary needs to be randomized and split into a training and test set. Here we will use an 80-20 train-test split; 80% of the data will be used to train and 20% will be reserved for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid copying the dictionary multiple times, we will randomize the list of keys (ie the image IDs) we made above.\n",
    "random.shuffle(img_ids)\n",
    "\n",
    "# print one out to double check\n",
    "print(\"The new first entry is:\", img_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can split the list into training and test sets based on the number of entries\n",
    "idx = 0.8*len(img_ids)\n",
    "\n",
    "train_ids = img_ids[0:int(idx)]  # this will copy all the image ids from 0 to the 80% cut-off\n",
    "test_ids = img_ids[int(idx)::]  # this will copy all the image ids from the cut-off to the end\n",
    "\n",
    "# double check\n",
    "print(\"cut off for 80-20 split:\", str(int(idx)))\n",
    "print(\"number of training images:\", str(len(train_ids)))\n",
    "print(\"nubmer of test images:\", str(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data split by the image ID, we can select the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train it, feed in the features and labels\n",
    "\n",
    "# pull out the features for the training data\n",
    "# the next line uses \"list comprehension\" to pull out the feature vectors only from the trianing data\n",
    "train_features = [data[line]['features'] for line in train_ids]\n",
    "train_features = np.asarray(train_features)  # convert to an array\n",
    "\n",
    "# retrieve the numeric classes of the training data\n",
    "train_labels = [data[line]['class'] for line in train_ids]\n",
    "train_labels = np.asarray(train_labels)  # convert to an array\n",
    "\n",
    "# check to make sure these numbers are right. We expect the training features to be a matrix with \n",
    "# dimensions [n_images x n_features] and the training labels to be a matrix with dimensions [n_images x 1]\n",
    "print(\"train features dim:\", train_features.shape)\n",
    "print(\"train labels dim:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test it, feed in the features and labels from the test data\n",
    "\n",
    "# pull out the features for the test data\n",
    "# the next line uses \"list comprehension\" to pull out the feature vectors only from the trianing data\n",
    "test_features = [data[line]['features'] for line in test_ids]\n",
    "test_features = np.asarray(test_features)  # convert to an array\n",
    "\n",
    "# retrieve the numeric classes of the training data\n",
    "test_labels = [data[line]['class'] for line in test_ids]\n",
    "test_labels = np.asarray(test_labels)  # convert to an array\n",
    "\n",
    "# check to make sure these numbers are right. We expect the test features to be a matrix with \n",
    "# dimensions [n_images x n_features] and the test labels to be a matrix with dimensions [n_images x 1]\n",
    "print(\"test features dim:\", test_features.shape)\n",
    "print(\"test labels dim:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features and labels are now divided into training and test sets that we can use multiple times. The order of these will remain the same and correspond witht the order of the image IDs. \n",
    "\n",
    "## Feature standardization\n",
    "\n",
    "It is good practice to standardize the features before feeding them into an ensemble or margin classifier. Here, standardization simply means making each feature look a Gaussian with zero mean and unit variance. SKLearn provides a class to fit a standardizer, save it, and apply it to both training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke an instance of the standardizer class and fit it to the training features\n",
    "scale_transform = preprocessing.StandardScaler().fit(train_features)\n",
    "\n",
    "# The scale_transform instance stores all the information we need for the transformer\n",
    "# print the mean of each feature\n",
    "print(\"mean of first feature:\", scale_transform.mean_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now that the data is imported into the workspace in an organized way and the transformer prepared, we can begin training and testing classifiers. The same training and test data will be used for the both margin and ensemble classifiers. \n",
    "\n",
    "## Margin classifiers\n",
    "\n",
    "A margin classifier seperates data in a space by assigning a distance between each point and the decision boundary. Imagine that we have just 2 features, $x_{1}$ and $x_{2}$, to seperate two classes. We can plot the points in a plane and find a line that seperates them.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg\">\n",
    "    <figcaption>\n",
    "        Points and hyperplanes. Courtesy: ZackWeinberg, via Wikipedia\n",
    "    </figcaption>\n",
    "</figure>\n",
    "        \n",
    "\n",
    "The line labeled $H_{3}$ is the best linear discriminant of this data. \n",
    "\n",
    "A classic and widely used margin classifier is the *support vector machine* (SVM). SVMs search a space, definied by the features, to find the optimal seperating hyperplane (ie a plane in many dimensions). In the example above, it would iteratively try many lines such as $H_{1}$ and $H_{2}$ before eventually settling on a particular plane.\n",
    "\n",
    "We will use the implimentation in SKLearn, svc -- a class that contains all functions need to train, test, and deplpoy a SVM. Do note however, that fitting a SVM is computationally expensive and scales quadratically. In other words, training an SVM with O(10k) samples becomes really time consuming and memory hungry.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create an instance of the SVM\n",
    "svm_clf = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernerl parameter tells SKLearn how to seperate the data. A 'linear' kernerl will attempt to find linear decision boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the SVM (this may take a few minutes)\n",
    "# the first parameter is the training features. Make sure to scale them!\n",
    "# the second parameter is the training labels. These do not need to be scaled\n",
    "svm_clf.fit(scale_transform.transform(train_features), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, the classifier will show a bunch of information about the classifier. We can now apply it to new data to see how accurate it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the new data through the classifier to get the mean accuracy\n",
    "# the first parameter is the test data. Remember to scale it\n",
    "acc_svm = svm_clf.score(scale_transform.transform(test_features), test_labels)\n",
    "\n",
    "print(\"Linear SVM accuracy:\", acc_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the accuracy of the classifier using a *confusion matrix* that compares the classifier labels to the true labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the confusion matrix, we need the predicitons from the classifier on an image-by-image basis.\n",
    "svm_preds = svm_clf.predict(scale_transform.transform(test_features))\n",
    "\n",
    "# feed the predictions and the true labels to a confusion matrix plotting utility\n",
    "make_confmat(test_labels, svm_preds, acc_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble classifiers\n",
    "\n",
    "Rather than relying on the results of a single classifer, ensemble classifier combine the results of many smaller classifiers. There are many ways of generating such a collection of computer classifiers. Here we will focus on the popular random forest (RFs) models. \n",
    "\n",
    "RFs build a collection of decision trees built from a random selection of the feature set. A single decision tree is a type of flow chart: each node in the tree is a test on a single feature and each branch denotes the outcome. A terminal node, or leaf, represents the tree's final classification.\n",
    "\n",
    "A single decsion tree tends to overfit the data -- it become really good at representing the training data but does not generalize well to new data. RFs get around this by creating many trees, using a random subsample of features and training examples for each tree. A new sample is then fed into every tree and the results averaged at the end to come to a final decision.\n",
    "\n",
    "To train a RF in python we will use the sklearn's ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we are only using a few of the RF parameters. There are many ways to modify this\n",
    "rf_clf = ensemble.RandomForestClassifier(n_estimators=30, n_jobs=8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not trained the classifier yet. The code above defines an instance *rf_clf* of the class *RandomForestClassifier*. The parameters we added define a few things:\n",
    "\n",
    "* n_estimators is the number of trees. For now we are just using 30\n",
    "* n_jobs parallalizes the process. It subdivides the process out to some number of cores. This speeds up training and is limited by the hardware you are working on. \n",
    "* verbose just tells sklearn they we want feedback as it is training. \n",
    "\n",
    "There are loads of other parameters that can change how the classifier behaves. For our purposes, mostly using defaults will suffice. Note that we are not exlicitly defining the number of features the will be used in each random tree. The default as set by sklearn is $\\sqrt{n\\_features}$. This is generally a good rule of thumb.\n",
    "\n",
    "To train the classifier we need to give it data. This is done with the *fit* method of the *RandomForestClassifier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug into the fit method. this step might take a little while.\n",
    "# much as with the SVM, be sure to scale the training features.\n",
    "rf_clf.fit(scale_transform.transform(train_features), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done! The classifier is trained. Now we can test it on the independent set that it has not seen yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug test data into the trained classifier\n",
    "acc = rf_clf.score(scale_transform.transform(test_features), test_labels)\n",
    "\n",
    "print(acc, '% correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. But we need a way of visualizing what classes it had trouble with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels for the test set from the classifier\n",
    "preds = rf_clf.predict(scale_transform.transform(test_features))\n",
    "\n",
    "# make a confusion matrix\n",
    "make_confmat(test_labels, preds, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ideal case, this matrix would be purely diagonal. The class at index 17 is particularly interesting. There seems to be a lot of data going there. Which one is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_names[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Mix is a mishmash class. It seems like one that the classifier could easily confuse. It is also a big class (~2k images) relative to some of the small ones. This means there is some variability in that might not have been captured in the training data for some of the spase classes. \n",
    "\n",
    "With the dictonaries, we can make a list of all the images labeled as \"Aggregate Mix\" and see a few to get a sense of what was confusing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indicies of images that were labeled as the \"Aggregate Mix\" class.\n",
    "# nonzero creates a list of booleans - True when the condition is met, false otherwise\n",
    "[mask, ] = np.nonzero(preds == 17)\n",
    "\n",
    "# convert the ndarray to a list for indexing\n",
    "mask = list(mask)\n",
    "\n",
    "# actually get the associated image ids\n",
    "labeled_as_skinny = [test_ids[item] for item in mask]\n",
    "len(labeled_as_skinny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the first 20 images labeled at skinny mix\n",
    "# first we need to retrieve the true classes from the image IDs\n",
    "true_numeric = [test_labels[item] for item in mask[0:20]]\n",
    "\n",
    "# now convert this to a list of strings with the true label\n",
    "true_str = [cls_names[item][1] for item in true_numeric]\n",
    "\n",
    "# now make a list of file paths by combining the image ID, label and base path\n",
    "labeled_imgs = [os.path.join(base_ptf, \"../SPC_manual_labels\", item[0], item[1]) \n",
    "                for item in zip(true_str, labeled_as_skinny[0:20])]\n",
    "\n",
    "# now tile the images using the image tiling utility\n",
    "tiled = tile_images(labeled_imgs, [2, 10])\n",
    "\n",
    "# display it\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.imshow(tiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very hard to tell some of these apart, even for a human. The classifier is mapping many of these skinny objects to this mixed class. \n",
    "\n",
    "## Excercises\n",
    "\n",
    "Complete the following for extra practice.\n",
    "\n",
    "1. Train and test a new SVM with a Radial Basis Function (RBF) kernel.\n",
    "A RBF kernel computes nonlinear decision boundaries in the feature space. This can be an effective way of representing more complex data types. Mathematically, this \"kernel trick\" projects the data to a higher dimensional space where it is linearly seperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new classifier with the kernel set to 'rbf'\n",
    "svm_rbf = svm.SVC()\n",
    "\n",
    "# train it on the same training data as above, being careful to scale the features.\n",
    "svm_rbf.fit()\n",
    "\n",
    "# compute the accuracy on the test dat\n",
    "rbf_acc = svm_rbf.score()\n",
    "\n",
    "# print out the result\n",
    "print(\"SVM with RBF mean accuracy=\", rbf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this classifier compare to the linear SVM? \n",
    "\n",
    "Be aware that there are many SVM parameters that we are not covering here. The RBF, for example, has two \"hyperparameters\", the $\\gamma$ term in particular, that must be tuned and tested. This is usually done with a process called \"cross-validation\" to optimize the best parameters for the classifier. \n",
    "\n",
    "2. Train a series of RFs with progressively more trees and evaluate the performance.\n",
    "The number of trees used for a random forest is an important hyperparamter. Intuitively, having more trees seems like it would lead to better results. Try it out and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a for-loop to iteratively train an RF with the following number of trees.\n",
    "num_trees = [5, 10, 20, 50, 100, 150, 200, 250]\n",
    "\n",
    "# initalize a list to hold all the scores\n",
    "acc_out = []\n",
    "\n",
    "# the for-loop\n",
    "for tr in num_trees:\n",
    "    \n",
    "    # instantiate the classifier\n",
    "    rf_temp = \n",
    "    \n",
    "    # train it with scaled features\n",
    "    rf_temp.fit()\n",
    "    \n",
    "    # test the trained classifier \n",
    "    acc_temp = rf_temp\n",
    "    \n",
    "    # save the accuracy\n",
    "    acc_out.append(acc_temp)\n",
    "    \n",
    "# plot the output\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "ax.plot(num_trees, acc_out)\n",
    "ax.xlabel('number of trees')\n",
    "ax.ylabel('mean test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the pattern? What seems to be a reasonable number of trees?\n",
    "\n",
    "In this example, it does not much matter since training takes such a short period of time. But with more training images, this can be a lengthy process that makes any computational savings worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
